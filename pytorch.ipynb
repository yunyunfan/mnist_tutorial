import torch
import torch.nn as nn
import torch.utils.data as data
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
from torch import optim
from torch.autograd import Variable
import time

BATCH_SIZE = 128
NUM_EPOCHS = 10
learning_rate = 0.02
# preprocessing
normalize = transforms.Normalize(mean=[.5], std=[.5])
transform = transforms.Compose([transforms.ToTensor(), normalize])

# download and load the data
train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=True)
test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)

# encapsulate them into dataloader form
train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)


test_x = torch.unsqueeze(test_dataset.test_data, dim=1).type(torch.FloatTensor)/255.
test_y = test_dataset.test_labels


class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)
            nn.Conv2d(
                in_channels=1,      # input height
                out_channels=16,    # n_filters
                kernel_size=5,      # filter size
                stride=1,           # filter movement/step
                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1
            ),      # output shape (16, 28, 28)
            nn.ReLU(),    # activation
            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)
        )
        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)
            nn.ReLU(),  # activation
            nn.MaxPool2d(2),  # output shape (32, 7, 7)
        )
        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)
        output = self.out(x)
        return output

cnn = CNN()
#print(cnn)  # net architecture

optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)   # optimize all cnn parameters
loss_func = nn.CrossEntropyLoss()   # the target label is not one-hotted

# training and testing
for epoch in range(NUM_EPOCHS):
    for step, (x, y) in enumerate(train_loader):   # 分配 batch data, normalize x when iterate train_loader
        b_x=Variable(x)
        b_y =Variable(y)

        output = cnn(b_x)
        loss = loss_func(output,b_y)   # cross entropy loss

        optimizer.zero_grad()           # clear gradients for this training step/usr/bin
        loss.backward()                 # backpropagation, compute gradients
        optimizer.step()                # apply gradients

        if step % 128 == 0:
            test_output = cnn (test_x)  
            testpred_y = torch.max (test_output, 1)[1].data.squeeze ()
            train_output = cnn (x)
            trainpred_y = torch.max (train_output, 1)[1].data.squeeze ()
            testaccuracy = (testpred_y == test_y).sum().numpy() / test_y.size(0)
            trainaccuracy = (trainpred_y == y).sum ().numpy () / y.size (0)
            print ('Epoch: ', epoch, '| train loss: %.4f' % loss.data[0], '| train accuracy: %.2f%%' % (trainaccuracy*100) ,'| test accuracy: %.2f%%' % (testaccuracy*100))

'''
Epoch:  1 | train loss: 0.0867 | train accuracy: 96.88% | test accuracy: 92.23%
Epoch:  2 | train loss: 0.1531 | train accuracy: 98.44% | test accuracy: 94.96%
Epoch:  3 | train loss: 0.0457 | train accuracy: 99.22% | test accuracy: 93.31%
Epoch:  4 | train loss: 0.0073 | train accuracy: 100.00% | test accuracy: 92.69%
Epoch:  5 | train loss: 0.1903 | train accuracy: 97.66% | test accuracy: 93.46%
Epoch:  6 | train loss: 0.0243 | train accuracy: 100.00% | test accuracy: 84.91%
Epoch:  7 | train loss: 0.0287 | train accuracy: 99.22% | test accuracy: 86.82%
Epoch:  8 | train loss: 0.0180 | train accuracy: 100.00% | test accuracy: 92.83%
Epoch:  9 | train loss: 0.0947 | train accuracy: 98.44% | test accuracy: 91.00%
'''