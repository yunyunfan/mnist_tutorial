import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_mldata
'''
a = np.array([[1,2,3], [2,3,4]])
print(a.ndim, a.shape, a.size, a.dtype, type(a))
#2 (2, 3) 6 int64 <class 'numpy.ndarray'>
b = np.zeros((3,4))
c = np.ones((3,4))
d = np.random.randn(2,3)
e = np.array([[1,2], [2,3], [3,4]])
f = b*2 - c*3
g = 2*c*f
h = np.dot(a,e)
i = d.mean()
j = d.max(axis=1)
k = a[-1][:2]
print(d)
print(j)
# You can print a to k for details

x = np.arange(2, 10, 0.2)

plt.plot(x, x**1.5*.5, 'r-', x, np.log(x)*5, 'g--', x, x, 'b.')
plt.show()

def f(x):
    return np.sin(np.pi*x)

x1 = np.arange(0, 5, 0.1)
x2 = np.arange(0, 5, 0.01)

plt.subplot(211)
plt.plot(x1, f(x1), 'go', x2, f(x2-1))

plt.subplot(212)
plt.plot(x2, f(x2), 'r--')
plt.show()

img = np.arange(0, 1, 1/32/32) # define an 1D array with 32x32 elements gradually increasing
img = img.reshape(32, 32) # reshape it into 32x32 array, the array represents a 32x32 image,
                          # each element represents the corresponding pixel of the image
plt.imshow(img, cmap='gray')
plt.show()
'''

# download and read mnist
mnist = fetch_mldata('MNIST original',data_home='/mnt/H/yunyun/minist/ministdata')

# 'mnist.data' is 70k x 784 array, each row represents the pixels from a 28x28=784 image
# 'mnist.target' is 70k x 1 array, each row represents the target class of the corresponding image
images = mnist.data
targets = mnist.target

# make the value of pixels from [0, 255] to [0, 1] for further process
X = mnist.data / 255.
Y = mnist.target
'''
# print the first image of the dataset
img1 = X[0].reshape(28, 28)
plt.imshow(img1, cmap='gray')
plt.show()

# print the images after simple transformation
img2 = 1 - img1
plt.imshow(img2, cmap='gray')
plt.show()

img3 = img1.transpose()
plt.imshow(img3, cmap='gray')
plt.show()
'''
# split data to train and test (for faster calculation, just use 1/10 data)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X[::10], Y[::10], test_size=1000)


#Q1
#use logistic regression
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
# 使用LogisticRegression考察线性回归的预测能力
cls = LogisticRegression()
# 把数据交给模型训练
cls.fit(X_train, y_train)
    #print("Coefficients:%s, intercept %s"%(cls.coef_,cls.intercept_))
    #print("Residual sum of squares: %.2f"% np.mean((cls.predict(X_test) - y_test) ** 2))
   # print('Score: %.2f' % cls.score(X_test, y_test))
train_accuracy = cls.score(X_train, y_train)
test_accuracy = cls.score(X_test, y_test)
print('Training accuracy: %0.2f%%' % (train_accuracy*100))
print('Testing accuracy: %0.2f%%' % (test_accuracy*100))

'''
# Q2 TODO:use naive bayes
from sklearn.naive_bayes import BernoulliNB
clf = BernoulliNB()
clf.fit (X_train, y_train)
train_accuracy = clf.score (X_train, y_train)
test_accuracy = clf.score (X_test, y_test)
print('Training accuracy: %0.2f%%' % (train_accuracy*100))
print('Testing accuracy: %0.2f%%' % (test_accuracy*100))
'''
'''
# Q3 TODO:use support vector machine
from sklearn.svm import LinearSVC
clf = LinearSVC()
clf.fit (X_train, y_train)
train_accuracy = clf.score (X_train, y_train)
test_accuracy = clf.score (X_test, y_test)
print('Training accuracy: %0.2f%%' % (train_accuracy*100))
print('Testing accuracy: %0.2f%%' % (test_accuracy*100))
'''
'''
# Q4 TODO:use SVM with another group of parameters
from sklearn import svm
clf = svm.SVC(C=10.0,kernel='rbf',gamma=0.001)
clf.fit (X_train, y_train)
train_accuracy = clf.score (X_train, y_train)
test_accuracy = clf.score (X_test, y_test)
print('Training accuracy: %0.2f%%' % (train_accuracy*100))
print('Testing accuracy: %0.2f%%' % (test_accuracy*100))
'''